Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_adani_48_48', data='custom', root_path='./dataset/finance', data_path='ADANIPORTS.csv', features='S', target='OT', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=30, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_adani_48_48_PathFormer_ftADANIPORTS_slS_pl48_48>>>>>>>>>>>>>>>>>>>>>>>>>>
         date       0       1        2      3      4       5      OT
0  27-11-2007  440.00  770.00  1050.00  770.0  959.0  984.72  962.90
1  28-11-2007  962.90  984.00   990.00  874.0  885.0  941.38  893.90
2  29-11-2007  893.90  909.00   914.75  841.0  887.0  888.09  884.20
3  30-11-2007  884.20  890.00   958.00  890.0  929.0  929.17  921.55
4  03-12-2007  921.55  939.75   995.00  922.0  980.0  965.65  969.30
train 2230
         date       0       1        2      3      4       5      OT
0  27-11-2007  440.00  770.00  1050.00  770.0  959.0  984.72  962.90
1  28-11-2007  962.90  984.00   990.00  874.0  885.0  941.38  893.90
2  29-11-2007  893.90  909.00   914.75  841.0  887.0  888.09  884.20
3  30-11-2007  884.20  890.00   958.00  890.0  929.0  929.17  921.55
4  03-12-2007  921.55  939.75   995.00  922.0  980.0  965.65  969.30
val 286
         date       0       1        2      3      4       5      OT
0  27-11-2007  440.00  770.00  1050.00  770.0  959.0  984.72  962.90
1  28-11-2007  962.90  984.00   990.00  874.0  885.0  941.38  893.90
2  29-11-2007  893.90  909.00   914.75  841.0  887.0  888.09  884.20
3  30-11-2007  884.20  890.00   958.00  890.0  929.0  929.17  921.55
4  03-12-2007  921.55  939.75   995.00  922.0  980.0  965.65  969.30
test 617
Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_adani_48_48', data='custom', root_path='./dataset/finance', data_path='ADANIPORTS.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=30, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_adani_48_48_PathFormer_ADANIPORTS_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...    Volume      Turnover   Close
0  27-11-2007      440.00  770.00  ...  27294366  2.687719e+15  962.90
1  28-11-2007      962.90  984.00  ...   4581338  4.312765e+14  893.90
2  29-11-2007      893.90  909.00  ...   5124121  4.550658e+14  884.20
3  30-11-2007      884.20  890.00  ...   4609762  4.283257e+14  921.55
4  03-12-2007      921.55  939.75  ...   2977470  2.875200e+14  969.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open     High  ...    Volume      Turnover    Close
0         440.00  770.00  1050.00  ...  27294366  2.687719e+15   962.90
1         962.90  984.00   990.00  ...   4581338  4.312765e+14   893.90
2         893.90  909.00   914.75  ...   5124121  4.550658e+14   884.20
3         884.20  890.00   958.00  ...   4609762  4.283257e+14   921.55
4         921.55  939.75   995.00  ...   2977470  2.875200e+14   969.30
...          ...     ...      ...  ...       ...           ...      ...
3317      725.35  733.00   739.65  ...   9390549  6.885658e+14  3653.75
3318      730.75  735.00   757.50  ...  20573107  1.538191e+15  3745.75
3319      749.15  755.00   760.00  ...  11156977  8.379106e+14  3731.25
3320      746.25  753.20   765.85  ...  13851910  1.043139e+15  3733.75
3321      746.75  739.00   759.45  ...  12600934  9.366911e+14  3650.25

[3322 rows x 9 columns]
after scaling the data 
 [[ 5.32388030e-01  2.02417799e+00  3.17464957e+00 ...  9.76878545e+00
   2.92206521e+01  1.82990289e-01]
 [ 2.91136557e+00  2.99455323e+00  2.90977727e+00 ...  9.68934116e-01
   4.17171090e+00  4.98248487e-04]
 [ 2.59744427e+00  2.65446845e+00  2.57758326e+00 ...  1.17922795e+00
   4.43579792e+00 -2.51564296e-02]
 ...
 [ 1.93889196e+00  1.95616103e+00  1.89443345e+00 ...  3.51657530e+00
   8.68578642e+00  7.50475608e+00]
 [ 1.92569817e+00  1.94799900e+00  1.92025850e+00 ...  4.56069015e+00
   1.09640391e+01  7.51136811e+00]
 [ 1.92797296e+00  1.88360961e+00  1.89200545e+00 ...  4.07601665e+00
   9.78235717e+00  7.29052629e+00]]
2325
train 2230
x data after scaling 
[[ 5.32388030e-01  2.02417799e+00  3.17464957e+00 ...  9.76878545e+00
   2.92206521e+01  1.82990289e-01]
 [ 2.91136557e+00  2.99455323e+00  2.90977727e+00 ...  9.68934116e-01
   4.17171090e+00  4.98248487e-04]
 [ 2.59744427e+00  2.65446845e+00  2.57758326e+00 ...  1.17922795e+00
   4.43579792e+00 -2.51564296e-02]
 ...
 [-1.35620657e-02 -8.84883536e-03  1.93582729e-02 ...  2.80308503e+00
   2.78243195e+00  1.85940449e+00]
 [-1.65192954e-02 -7.26177304e-03 -1.41922187e-02 ...  8.99866236e-01
   9.69300159e-01  1.94469968e+00]
 [ 1.28255223e-02  1.74510544e-02  1.03084692e-02 ...  9.14135902e-01
   1.01576087e+00  2.03197848e+00]]
len of sample data 9

2325
2325
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...    Volume      Turnover   Close
0  27-11-2007      440.00  770.00  ...  27294366  2.687719e+15  962.90
1  28-11-2007      962.90  984.00  ...   4581338  4.312765e+14  893.90
2  29-11-2007      893.90  909.00  ...   5124121  4.550658e+14  884.20
3  30-11-2007      884.20  890.00  ...   4609762  4.283257e+14  921.55
4  03-12-2007      921.55  939.75  ...   2977470  2.875200e+14  969.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open     High  ...    Volume      Turnover    Close
0         440.00  770.00  1050.00  ...  27294366  2.687719e+15   962.90
1         962.90  984.00   990.00  ...   4581338  4.312765e+14   893.90
2         893.90  909.00   914.75  ...   5124121  4.550658e+14   884.20
3         884.20  890.00   958.00  ...   4609762  4.283257e+14   921.55
4         921.55  939.75   995.00  ...   2977470  2.875200e+14   969.30
...          ...     ...      ...  ...       ...           ...      ...
3317      725.35  733.00   739.65  ...   9390549  6.885658e+14  3653.75
3318      730.75  735.00   757.50  ...  20573107  1.538191e+15  3745.75
3319      749.15  755.00   760.00  ...  11156977  8.379106e+14  3731.25
3320      746.25  753.20   765.85  ...  13851910  1.043139e+15  3733.75
3321      746.75  739.00   759.45  ...  12600934  9.366911e+14  3650.25

[3322 rows x 9 columns]
after scaling the data 
 [[ 5.32388030e-01  2.02417799e+00  3.17464957e+00 ...  9.76878545e+00
   2.92206521e+01  1.82990289e-01]
 [ 2.91136557e+00  2.99455323e+00  2.90977727e+00 ...  9.68934116e-01
   4.17171090e+00  4.98248487e-04]
 [ 2.59744427e+00  2.65446845e+00  2.57758326e+00 ...  1.17922795e+00
   4.43579792e+00 -2.51564296e-02]
 ...
 [ 1.93889196e+00  1.95616103e+00  1.89443345e+00 ...  3.51657530e+00
   8.68578642e+00  7.50475608e+00]
 [ 1.92569817e+00  1.94799900e+00  1.92025850e+00 ...  4.56069015e+00
   1.09640391e+01  7.51136811e+00]
 [ 1.92797296e+00  1.88360961e+00  1.89200545e+00 ...  4.07601665e+00
   9.78235717e+00  7.29052629e+00]]
381
val 286
x data after scaling 
[[-0.08794777 -0.07958647 -0.10579389 ... -0.19350222 -0.08255057
   1.67691245]
 [-0.07930356 -0.07754596 -0.10049644 ... -0.01867741  0.07051452
   1.6669944 ]
 [-0.08271574 -0.08162698 -0.07577503 ...  1.25242551  1.21858575
   1.75228959]
 ...
 [ 0.22824833  0.21946141  0.19086309 ...  0.11467971  0.3616912
   2.53317039]
 [ 0.21528202  0.21492695  0.21293578 ... -0.03765675  0.21086087
   2.58342182]
 [ 0.23257044  0.23941305  0.2147016  ... -0.30993535 -0.07975199
   2.63830167]]
len of sample data 9

381
381
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...    Volume      Turnover   Close
0  27-11-2007      440.00  770.00  ...  27294366  2.687719e+15  962.90
1  28-11-2007      962.90  984.00  ...   4581338  4.312765e+14  893.90
2  29-11-2007      893.90  909.00  ...   5124121  4.550658e+14  884.20
3  30-11-2007      884.20  890.00  ...   4609762  4.283257e+14  921.55
4  03-12-2007      921.55  939.75  ...   2977470  2.875200e+14  969.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open     High  ...    Volume      Turnover    Close
0         440.00  770.00  1050.00  ...  27294366  2.687719e+15   962.90
1         962.90  984.00   990.00  ...   4581338  4.312765e+14   893.90
2         893.90  909.00   914.75  ...   5124121  4.550658e+14   884.20
3         884.20  890.00   958.00  ...   4609762  4.283257e+14   921.55
4         921.55  939.75   995.00  ...   2977470  2.875200e+14   969.30
...          ...     ...      ...  ...       ...           ...      ...
3317      725.35  733.00   739.65  ...   9390549  6.885658e+14  3653.75
3318      730.75  735.00   757.50  ...  20573107  1.538191e+15  3745.75
3319      749.15  755.00   760.00  ...  11156977  8.379106e+14  3731.25
3320      746.25  753.20   765.85  ...  13851910  1.043139e+15  3733.75
3321      746.75  739.00   759.45  ...  12600934  9.366911e+14  3650.25

[3322 rows x 9 columns]
after scaling the data 
 [[ 5.32388030e-01  2.02417799e+00  3.17464957e+00 ...  9.76878545e+00
   2.92206521e+01  1.82990289e-01]
 [ 2.91136557e+00  2.99455323e+00  2.90977727e+00 ...  9.68934116e-01
   4.17171090e+00  4.98248487e-04]
 [ 2.59744427e+00  2.65446845e+00  2.57758326e+00 ...  1.17922795e+00
   4.43579792e+00 -2.51564296e-02]
 ...
 [ 1.93889196e+00  1.95616103e+00  1.89443345e+00 ...  3.51657530e+00
   8.68578642e+00  7.50475608e+00]
 [ 1.92569817e+00  1.94799900e+00  1.92025850e+00 ...  4.56069015e+00
   1.09640391e+01  7.51136811e+00]
 [ 1.92797296e+00  1.88360961e+00  1.89200545e+00 ...  4.07601665e+00
   9.78235717e+00  7.29052629e+00]]
712
test 617
x data after scaling 
[[ 0.29990428  0.3062964   0.26988333 ...  0.17383813  0.469224
   2.69913235]
 [ 0.2723793   0.27228792  0.23302193 ...  0.12309483  0.38466599
   2.58805024]
 [ 0.23416279  0.20948559  0.19880926 ...  2.01401639  2.39779271
   2.56821415]
 ...
 [ 1.93889196  1.95616103  1.89443345 ...  3.5165753   8.68578642
   7.50475608]
 [ 1.92569817  1.947999    1.9202585  ...  4.56069015 10.96403911
   7.51136811]
 [ 1.92797296  1.88360961  1.89200545 ...  4.07601665  9.78235717
   7.29052629]]
len of sample data 9

2325
2325
2325
2325
Epoch: 1 cost time: 149.5182921886444
381
381
381
712
Epoch: 1, Steps: 69 | Train Loss: 0.4342866 Vali Loss: 0.3915166 Test Loss: 0.6808662
Validation loss decreased (inf --> 0.391517).  Saving model ...
Updating learning rate to 5.639495449673965e-05
2325
2325
2325
Epoch: 2 cost time: 129.86745643615723
381
381
381
712
Epoch: 2, Steps: 69 | Train Loss: 0.3179294 Vali Loss: 0.3134308 Test Loss: 0.5685359
Validation loss decreased (0.391517 --> 0.313431).  Saving model ...
Updating learning rate to 0.00010445984076633307
2325
2325
2325
Epoch: 3 cost time: 111.54756212234497
381
381
381
712
Epoch: 3, Steps: 69 | Train Loss: 0.2729712 Vali Loss: 0.2873831 Test Loss: 0.5254122
Validation loss decreased (0.313431 --> 0.287383).  Saving model ...
Updating learning rate to 0.00018091123537844294
2325
2325
2325
Epoch: 4 cost time: 104.95753645896912
381
381
381
712
Epoch: 4, Steps: 69 | Train Loss: 0.2493971 Vali Loss: 0.2703855 Test Loss: 0.5066140
Validation loss decreased (0.287383 --> 0.270386).  Saving model ...
Updating learning rate to 0.0002805265669335782
2325
2325
2325
Epoch: 5 cost time: 98.3646125793457
381
381
381
712
Epoch: 5, Steps: 69 | Train Loss: 0.2352799 Vali Loss: 0.2586682 Test Loss: 0.4957660
Validation loss decreased (0.270386 --> 0.258668).  Saving model ...
Updating learning rate to 0.0003965008819816387
2325
2325
2325
Epoch: 6 cost time: 123.4269483089447
381
381
381
712
Epoch: 6, Steps: 69 | Train Loss: 0.2263043 Vali Loss: 0.2570088 Test Loss: 0.4983357
Validation loss decreased (0.258668 --> 0.257009).  Saving model ...
Updating learning rate to 0.0005209117071142762
2325
2325
2325
Epoch: 7 cost time: 125.23701596260071
381
381
381
712
Epoch: 7, Steps: 69 | Train Loss: 0.2220893 Vali Loss: 0.2515694 Test Loss: 0.4797164
Validation loss decreased (0.257009 --> 0.251569).  Saving model ...
Updating learning rate to 0.0006452602514275266
2325
2325
2325
Epoch: 8 cost time: 174.8055272102356
381
381
381
712
Epoch: 8, Steps: 69 | Train Loss: 0.2173923 Vali Loss: 0.2507612 Test Loss: 0.4901365
Validation loss decreased (0.251569 --> 0.250761).  Saving model ...
Updating learning rate to 0.0007610519785640918
2325
2325
2325
Epoch: 9 cost time: 113.45048880577087
381
381
381
712
Epoch: 9, Steps: 69 | Train Loss: 0.2140594 Vali Loss: 0.2553846 Test Loss: 0.4889220
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0008603768881177095
2325
2325
2325
Epoch: 10 cost time: 98.33392858505249
381
381
381
712
Epoch: 10, Steps: 69 | Train Loss: 0.2116637 Vali Loss: 0.2546697 Test Loss: 0.4713351
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0009364498660361952
2325
2325
2325
Epoch: 11 cost time: 114.87536478042603
381
381
381
712
Epoch: 11, Steps: 69 | Train Loss: 0.2096586 Vali Loss: 0.2590877 Test Loss: 0.4860678
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0009840741914386945
2325
2325
2325
Epoch: 12 cost time: 136.47201466560364
381
381
381
712
Epoch: 12, Steps: 69 | Train Loss: 0.2066906 Vali Loss: 0.2533449 Test Loss: 0.4938080
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0009999984004618835
2325
2325
2325
Epoch: 13 cost time: 141.98873734474182
381
381
381
712
Epoch: 13, Steps: 69 | Train Loss: 0.2052601 Vali Loss: 0.2545552 Test Loss: 0.4866276
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0009921827144729438
2325
2325
2325
Epoch: 14 cost time: 202.59835767745972
381
381
381
712
Epoch: 14, Steps: 69 | Train Loss: 0.2009702 Vali Loss: 0.2519247 Test Loss: 0.4693579
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0009694123665302025
2325
2325
2325
Epoch: 15 cost time: 266.6036114692688
381
381
381
712
Epoch: 15, Steps: 69 | Train Loss: 0.1998682 Vali Loss: 0.2594475 Test Loss: 0.4634732
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0009323792221335476
2325
2325
2325
Epoch: 16 cost time: 280.1167788505554
381
381
381
712
Epoch: 16, Steps: 69 | Train Loss: 0.1953562 Vali Loss: 0.2637272 Test Loss: 0.4763723
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0008822085146357961
2325
2325
2325
Epoch: 17 cost time: 165.84788966178894
381
381
381
712
Epoch: 17, Steps: 69 | Train Loss: 0.1916004 Vali Loss: 0.2606526 Test Loss: 0.4634033
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0008204246555966646
2325
2325
2325
Epoch: 18 cost time: 199.38689136505127
381
381
381
712
Epoch: 18, Steps: 69 | Train Loss: 0.1859209 Vali Loss: 0.2612436 Test Loss: 0.4684620
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : finance_adani_48_48_PathFormer_ADANIPORTS_ftMS_sl48_pl48_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...    Volume      Turnover   Close
0  27-11-2007      440.00  770.00  ...  27294366  2.687720e+15  962.90
1  28-11-2007      962.90  984.00  ...   4581338  4.312770e+14  893.90
2  29-11-2007      893.90  909.00  ...   5124121  4.550660e+14  884.20
3  30-11-2007      884.20  890.00  ...   4609762  4.283260e+14  921.55
4  03-12-2007      921.55  939.75  ...   2977470  2.875200e+14  969.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open     High  ...    Volume      Turnover    Close
0         440.00  770.00  1050.00  ...  27294366  2.687720e+15   962.90
1         962.90  984.00   990.00  ...   4581338  4.312770e+14   893.90
2         893.90  909.00   914.75  ...   5124121  4.550660e+14   884.20
3         884.20  890.00   958.00  ...   4609762  4.283260e+14   921.55
4         921.55  939.75   995.00  ...   2977470  2.875200e+14   969.30
...          ...     ...      ...  ...       ...           ...      ...
3317      725.35  733.00   739.65  ...   9390549  6.885660e+14  3653.75
3318      730.75  735.00   757.50  ...  20573107  1.538190e+15  3745.75
3319      749.15  755.00   760.00  ...  11156977  8.379110e+14  3731.25
3320      746.25  753.20   765.85  ...  13851910  1.043140e+15  3733.75
3321      746.75  739.00   759.45  ...  12600934  9.366910e+14  3650.25

[3322 rows x 9 columns]
after scaling the data 
 [[ 5.32388030e-01  2.02417799e+00  3.17464957e+00 ...  9.76878545e+00
   2.92205832e+01  1.82990289e-01]
 [ 2.91136557e+00  2.99455323e+00  2.90977727e+00 ...  9.68934116e-01
   4.17171312e+00  4.98248487e-04]
 [ 2.59744427e+00  2.65446845e+00  2.57758326e+00 ...  1.17922795e+00
   4.43579582e+00 -2.51564296e-02]
 ...
 [ 1.93889196e+00  1.95616103e+00  1.89443345e+00 ...  3.51657530e+00
   8.68577451e+00  7.50475608e+00]
 [ 1.92569817e+00  1.94799900e+00  1.92025850e+00 ...  4.56069015e+00
   1.09640304e+01  7.51136811e+00]
 [ 1.92797296e+00  1.88360961e+00  1.89200545e+00 ...  4.07601665e+00
   9.78233551e+00  7.29052629e+00]]
712
test 617
x data after scaling 
[[ 0.29990428  0.3062964   0.26988333 ...  0.17383813  0.4697707
   2.69913235]
 [ 0.2723793   0.27228792  0.23302193 ...  0.12309483  0.38429268
   2.58805024]
 [ 0.23416279  0.20948559  0.19880926 ...  2.01401639  2.3977998
   2.56821415]
 ...
 [ 1.93889196  1.95616103  1.89443345 ...  3.5165753   8.68577451
   7.50475608]
 [ 1.92569817  1.947999    1.9202585  ...  4.56069015 10.9640304
   7.51136811]
 [ 1.92797296  1.88360961  1.89200545 ...  4.07601665  9.78233551
   7.29052629]]
len of sample data 9

712
(608, 48, 1)
(608, 48, 1)
2.0691219717264175
mse:2476.313232421875, mae:37.09025955200195, rse:0.18988873064517975 ,mape : 2.0691219717264175
Inference time:  58.11154055595398
