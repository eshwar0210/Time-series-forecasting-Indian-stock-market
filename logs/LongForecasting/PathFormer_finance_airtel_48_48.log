Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_airtel_48_48', data='custom', root_path='./dataset/finance', data_path='BHARTIARTL.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=15, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_airtel_48_48_PathFormer_BHARTIARTL_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open   High  ...   VWAP    Volume      Turnover  Close
0  18-02-2002        0.00  51.90  51.90  ...  46.00  10381190  4.775430e+13  44.35
1  19-02-2002       44.35  45.15  45.15  ...  42.58   3552660  1.512610e+13  41.70
2  20-02-2002       41.70  40.60  42.80  ...  41.56   2512964  1.044350e+13  41.25
3  21-02-2002       41.25  42.85  43.40  ...  42.76   1338196  5.722680e+12  42.40
4  22-02-2002       42.40  42.65  43.60  ...  42.75    811327  3.468410e+12  43.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open    High  ...    Volume      Turnover    Close
0           0.00   51.90   51.90  ...  10381190  4.775430e+13    44.35
1          44.35   45.15   45.15  ...   3552660  1.512610e+13    41.70
2          41.70   40.60   42.80  ...   2512964  1.044350e+13    41.25
3          41.25   42.85   43.40  ...   1338196  5.722680e+12    42.40
4          42.40   42.65   43.60  ...    811327  3.468410e+12    43.30
...          ...     ...     ...  ...       ...           ...      ...
4769      522.60  522.75  528.50  ...   5819893  3.061970e+14  1055.10
4770      527.55  527.45  538.55  ...   9693047  5.180450e+14  1068.60
4771      534.30  535.95  548.00  ...  11970652  6.497870e+14  1088.60
4772      544.30  549.70  550.00  ...   8683857  4.713690e+14  1081.70
4773      540.85  533.50  547.00  ...   8178192  4.423890e+14  1073.50

[4774 rows x 9 columns]
after scaling the data 
 [[-1.53664815 -1.32199491 -1.32211574 ...  1.66677961 -0.55629415
  -1.69510463]
 [-1.35280166 -1.34990995 -1.34945322 ... -0.02515798 -0.74915713
  -1.70452151]
 [-1.36378685 -1.36872675 -1.35897071 ... -0.28276844 -0.77683565
  -1.70612061]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
3341
train 3246
x data after scaling 
[[-1.53664815 -1.32199491 -1.32211574 ...  1.66677961 -0.55629415
  -1.69510463]
 [-1.35280166 -1.34990995 -1.34945322 ... -0.02515798 -0.74915713
  -1.70452151]
 [-1.36378685 -1.36872675 -1.35897071 ... -0.28276844 -0.77683565
  -1.70612061]
 ...
 [ 0.26202136  0.26647387  0.23348794 ...  0.37323365  0.44818375
   1.12729664]
 [ 0.20149918  0.20857602  0.19663297 ... -0.28657897 -0.21262267
   1.15856781]
 [ 0.21973874  0.21560647  0.19096298 ... -0.56321408 -0.49283677
   1.14861789]]
len of sample data 9

3341
3341
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open   High  ...   VWAP    Volume      Turnover  Close
0  18-02-2002        0.00  51.90  51.90  ...  46.00  10381190  4.775430e+13  44.35
1  19-02-2002       44.35  45.15  45.15  ...  42.58   3552660  1.512610e+13  41.70
2  20-02-2002       41.70  40.60  42.80  ...  41.56   2512964  1.044350e+13  41.25
3  21-02-2002       41.25  42.85  43.40  ...  42.76   1338196  5.722680e+12  42.40
4  22-02-2002       42.40  42.65  43.60  ...  42.75    811327  3.468410e+12  43.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open    High  ...    Volume      Turnover    Close
0           0.00   51.90   51.90  ...  10381190  4.775430e+13    44.35
1          44.35   45.15   45.15  ...   3552660  1.512610e+13    41.70
2          41.70   40.60   42.80  ...   2512964  1.044350e+13    41.25
3          41.25   42.85   43.40  ...   1338196  5.722680e+12    42.40
4          42.40   42.65   43.60  ...    811327  3.468410e+12    43.30
...          ...     ...     ...  ...       ...           ...      ...
4769      522.60  522.75  528.50  ...   5819893  3.061970e+14  1055.10
4770      527.55  527.45  538.55  ...   9693047  5.180450e+14  1068.60
4771      534.30  535.95  548.00  ...  11970652  6.497870e+14  1088.60
4772      544.30  549.70  550.00  ...   8683857  4.713690e+14  1081.70
4773      540.85  533.50  547.00  ...   8178192  4.423890e+14  1073.50

[4774 rows x 9 columns]
after scaling the data 
 [[-1.53664815 -1.32199491 -1.32211574 ...  1.66677961 -0.55629415
  -1.69510463]
 [-1.35280166 -1.34990995 -1.34945322 ... -0.02515798 -0.74915713
  -1.70452151]
 [-1.36378685 -1.36872675 -1.35897071 ... -0.28276844 -0.77683565
  -1.70612061]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
527
val 432
x data after scaling 
[[ 0.09517082  0.10518701  0.08768806 ...  0.51744453  0.51277248
   0.97556036]
 [ 0.11299585  0.10518701  0.09882555 ...  0.16636797  0.18209145
   0.96312297]
 [ 0.10574148  0.10311923  0.08505556 ...  0.64224448  0.61880273
   0.93007502]
 ...
 [-0.0242154  -0.03128649 -0.03887433 ... -0.48378615 -0.46994197
   0.75914962]
 [-0.01323021 -0.01164257 -0.03036934 ... -0.33510096 -0.33900307
   0.75275324]
 [-0.01696103 -0.02136114 -0.04110183 ... -0.24760251 -0.26285907
   0.7577282 ]]
len of sample data 9

527
527
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open   High  ...   VWAP    Volume      Turnover  Close
0  18-02-2002        0.00  51.90  51.90  ...  46.00  10381190  4.775430e+13  44.35
1  19-02-2002       44.35  45.15  45.15  ...  42.58   3552660  1.512610e+13  41.70
2  20-02-2002       41.70  40.60  42.80  ...  41.56   2512964  1.044350e+13  41.25
3  21-02-2002       41.25  42.85  43.40  ...  42.76   1338196  5.722680e+12  42.40
4  22-02-2002       42.40  42.65  43.60  ...  42.75    811327  3.468410e+12  43.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open    High  ...    Volume      Turnover    Close
0           0.00   51.90   51.90  ...  10381190  4.775430e+13    44.35
1          44.35   45.15   45.15  ...   3552660  1.512610e+13    41.70
2          41.70   40.60   42.80  ...   2512964  1.044350e+13    41.25
3          41.25   42.85   43.40  ...   1338196  5.722680e+12    42.40
4          42.40   42.65   43.60  ...    811327  3.468410e+12    43.30
...          ...     ...     ...  ...       ...           ...      ...
4769      522.60  522.75  528.50  ...   5819893  3.061970e+14  1055.10
4770      527.55  527.45  538.55  ...   9693047  5.180450e+14  1068.60
4771      534.30  535.95  548.00  ...  11970652  6.497870e+14  1088.60
4772      544.30  549.70  550.00  ...   8683857  4.713690e+14  1081.70
4773      540.85  533.50  547.00  ...   8178192  4.423890e+14  1073.50

[4774 rows x 9 columns]
after scaling the data 
 [[-1.53664815 -1.32199491 -1.32211574 ...  1.66677961 -0.55629415
  -1.69510463]
 [-1.35280166 -1.34990995 -1.34945322 ... -0.02515798 -0.74915713
  -1.70452151]
 [-1.36378685 -1.36872675 -1.35897071 ... -0.28276844 -0.77683565
  -1.70612061]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
1002
test 907
x data after scaling 
[[-0.08245764 -0.08918433 -0.10063678 ...  0.5583685   0.35880459
   0.56548154]
 [-0.12619113 -0.11896037 -0.14194674 ... -0.43887325 -0.45943175
   0.57294398]
 [-0.12183851 -0.13053994 -0.14174424 ...  0.47259407  0.27768891
   0.55517627]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
len of sample data 9

3341
3341
3341
3341
	iters: 100, epoch: 1 | loss: 0.2494033
	speed: 1.9346s/iter; left time: 2739.3335s
Epoch: 1 cost time: 195.5934717655182
527
527
527
1002
Epoch: 1, Steps: 101 | Train Loss: 0.2693605 Vali Loss: 0.1939620 Test Loss: 0.3228408
Validation loss decreased (inf --> 0.193962).  Saving model ...
Updating learning rate to 0.00010451567044112982
3341
3341
3341
	iters: 100, epoch: 2 | loss: 0.1641734
	speed: 2.1912s/iter; left time: 2881.4458s
Epoch: 2 cost time: 134.0871124267578
527
527
527
1002
Epoch: 2, Steps: 101 | Train Loss: 0.1872268 Vali Loss: 0.1599512 Test Loss: 0.2733208
Validation loss decreased (0.193962 --> 0.159951).  Saving model ...
Updating learning rate to 0.00028071988287923404
3341
3341
3341
	iters: 100, epoch: 3 | loss: 0.1217926
	speed: 2.2315s/iter; left time: 2709.0209s
Epoch: 3 cost time: 141.63790845870972
527
527
527
1002
Epoch: 3, Steps: 101 | Train Loss: 0.1604494 Vali Loss: 0.1568831 Test Loss: 0.2637714
Validation loss decreased (0.159951 --> 0.156883).  Saving model ...
Updating learning rate to 0.0005212462502309978
3341
3341
3341
	iters: 100, epoch: 4 | loss: 0.1484342
	speed: 2.3474s/iter; left time: 2612.6580s
Epoch: 4 cost time: 150.94136953353882
527
527
527
1002
Epoch: 4, Steps: 101 | Train Loss: 0.1497778 Vali Loss: 0.1479807 Test Loss: 0.2569472
Validation loss decreased (0.156883 --> 0.147981).  Saving model ...
Updating learning rate to 0.0007614376064611356
3341
3341
3341
	iters: 100, epoch: 5 | loss: 0.1451921
	speed: 2.4722s/iter; left time: 2501.8976s
Epoch: 5 cost time: 160.09321975708008
527
527
527
1002
Epoch: 5, Steps: 101 | Train Loss: 0.1442035 Vali Loss: 0.1525758 Test Loss: 0.2555909
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0009367268416473491
3341
3341
3341
	iters: 100, epoch: 6 | loss: 0.1567976
	speed: 2.5981s/iter; left time: 2366.8758s
Epoch: 6 cost time: 165.6248745918274
527
527
527
1002
Epoch: 6, Steps: 101 | Train Loss: 0.1404762 Vali Loss: 0.1546192 Test Loss: 0.2581162
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.000999997013862385
3341
3341
3341
	iters: 100, epoch: 7 | loss: 0.1295045
	speed: 2.2095s/iter; left time: 1789.7206s
Epoch: 7 cost time: 132.68641591072083
527
527
527
1002
Epoch: 7, Steps: 101 | Train Loss: 0.1380647 Vali Loss: 0.1580457 Test Loss: 0.2618016
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0009692526010108774
3341
3341
3341
	iters: 100, epoch: 8 | loss: 0.1609735
	speed: 2.0383s/iter; left time: 1445.1766s
Epoch: 8 cost time: 125.58502745628357
527
527
527
1002
Epoch: 8, Steps: 101 | Train Loss: 0.1357774 Vali Loss: 0.1515060 Test Loss: 0.2559632
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0008819096402761631
3341
3341
3341
	iters: 100, epoch: 9 | loss: 0.1283118
	speed: 2.2093s/iter; left time: 1343.2549s
Epoch: 9 cost time: 138.5449059009552
527
527
527
1002
Epoch: 9, Steps: 101 | Train Loss: 0.1339424 Vali Loss: 0.1544036 Test Loss: 0.2630231
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0007485029817676619
3341
3341
3341
	iters: 100, epoch: 10 | loss: 0.1467994
	speed: 2.4839s/iter; left time: 1259.3558s
Epoch: 10 cost time: 162.45050621032715
527
527
527
1002
Epoch: 10, Steps: 101 | Train Loss: 0.1317502 Vali Loss: 0.1521569 Test Loss: 0.2640527
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0005851234373740878
3341
3341
3341
	iters: 100, epoch: 11 | loss: 0.1392421
	speed: 2.4390s/iter; left time: 990.2331s
Epoch: 11 cost time: 110.72367811203003
527
527
527
1002
Epoch: 11, Steps: 101 | Train Loss: 0.1303353 Vali Loss: 0.1588327 Test Loss: 0.2662474
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0004114769913745784
3341
3341
3341
	iters: 100, epoch: 12 | loss: 0.1227970
	speed: 2.2998s/iter; left time: 701.4497s
Epoch: 12 cost time: 120.010094165802
527
527
527
1002
Epoch: 12, Steps: 101 | Train Loss: 0.1275937 Vali Loss: 0.1537424 Test Loss: 0.2630666
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00024850796790527695
3341
3341
3341
	iters: 100, epoch: 13 | loss: 0.0980685
	speed: 2.5791s/iter; left time: 526.1278s
Epoch: 13 cost time: 145.021874666214
527
527
527
1002
Epoch: 13, Steps: 101 | Train Loss: 0.1260318 Vali Loss: 0.1521941 Test Loss: 0.2632956
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00011587283636321036
3341
3341
3341
	iters: 100, epoch: 14 | loss: 0.1031055
	speed: 2.3253s/iter; left time: 239.5045s
Epoch: 14 cost time: 125.30542373657227
527
527
527
1002
Epoch: 14, Steps: 101 | Train Loss: 0.1253881 Vali Loss: 0.1542825 Test Loss: 0.2640651
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : finance_airtel_48_48_PathFormer_BHARTIARTL_ftMS_sl48_pl48_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open   High  ...   VWAP    Volume      Turnover  Close
0  18-02-2002        0.00  51.90  51.90  ...  46.00  10381190  4.775430e+13  44.35
1  19-02-2002       44.35  45.15  45.15  ...  42.58   3552660  1.512610e+13  41.70
2  20-02-2002       41.70  40.60  42.80  ...  41.56   2512964  1.044350e+13  41.25
3  21-02-2002       41.25  42.85  43.40  ...  42.76   1338196  5.722680e+12  42.40
4  22-02-2002       42.40  42.65  43.60  ...  42.75    811327  3.468410e+12  43.30

[5 rows x 10 columns]
after adjusting cols 
      Prev Close    Open    High  ...    Volume      Turnover    Close
0           0.00   51.90   51.90  ...  10381190  4.775430e+13    44.35
1          44.35   45.15   45.15  ...   3552660  1.512610e+13    41.70
2          41.70   40.60   42.80  ...   2512964  1.044350e+13    41.25
3          41.25   42.85   43.40  ...   1338196  5.722680e+12    42.40
4          42.40   42.65   43.60  ...    811327  3.468410e+12    43.30
...          ...     ...     ...  ...       ...           ...      ...
4769      522.60  522.75  528.50  ...   5819893  3.061970e+14  1055.10
4770      527.55  527.45  538.55  ...   9693047  5.180450e+14  1068.60
4771      534.30  535.95  548.00  ...  11970652  6.497870e+14  1088.60
4772      544.30  549.70  550.00  ...   8683857  4.713690e+14  1081.70
4773      540.85  533.50  547.00  ...   8178192  4.423890e+14  1073.50

[4774 rows x 9 columns]
after scaling the data 
 [[-1.53664815 -1.32199491 -1.32211574 ...  1.66677961 -0.55629415
  -1.69510463]
 [-1.35280166 -1.34990995 -1.34945322 ... -0.02515798 -0.74915713
  -1.70452151]
 [-1.36378685 -1.36872675 -1.35897071 ... -0.28276844 -0.77683565
  -1.70612061]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
1002
test 907
x data after scaling 
[[-0.08245764 -0.08918433 -0.10063678 ...  0.5583685   0.35880459
   0.56548154]
 [-0.12619113 -0.11896037 -0.14194674 ... -0.43887325 -0.45943175
   0.57294398]
 [-0.12183851 -0.13053994 -0.14174424 ...  0.47259407  0.27768891
   0.55517627]
 ...
 [ 0.678215    0.67982314  0.68708756 ...  2.06060823  3.00227855
   2.0156823 ]
 [ 0.71966855  0.7366871   0.69518755 ...  1.24622328  1.94766238
   1.99116286]
 [ 0.70536707  0.66969102  0.68303756 ...  1.12093224  1.77636365
   1.96202381]]
len of sample data 9

1002
(896, 48, 1)
(896, 48, 1)
2.266274020075798
mse:696.7202758789062, mae:19.36025619506836, rse:0.16050346195697784 ,mape : 2.266274020075798
Inference time:  56.69987177848816
