Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_bajajauto_48_48', data='custom', root_path='./dataset/finance', data_path='BAJAJ-AUTO.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=30, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_bajajauto_48_48_PathFormer_BAJAJ-AUTO_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open   High  ...    VWAP   Volume      Turnover   Close
0  26-05-2008     2101.05  898.00  898.0  ...  624.61  3972485  2.481245e+14  604.75
1  27-05-2008      604.75  624.70  639.0  ...  606.43  1751063  1.061901e+14  593.15
2  28-05-2008      593.15  561.65  621.9  ...  608.75  1652355  1.005877e+14  608.15
3  29-05-2008      608.15  619.40  619.4  ...  600.98   669269  4.022170e+13  599.45
4  30-05-2008      599.45  605.40  607.0  ...  565.55  1262117  7.137940e+13  571.70

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover    Close
0        2101.05   898.00   898.00  ...  3972485  2.481245e+14   604.75
1         604.75   624.70   639.00  ...  1751063  1.061901e+14   593.15
2         593.15   561.65   621.90  ...  1652355  1.005877e+14   608.15
3         608.15   619.40   619.40  ...   669269  4.022170e+13   599.45
4         599.45   605.40   607.00  ...  1262117  7.137940e+13   571.70
...          ...      ...      ...  ...      ...           ...      ...
3197     3672.50  3682.20  3749.00  ...   480016  1.785560e+14  7477.70
3198     3738.85  3738.85  3798.00  ...   518487  1.952048e+14  7571.00
3199     3785.50  3810.05  3954.00  ...  1252958  4.899048e+14  7779.50
3200     3889.75  3943.00  3954.30  ...  1335444  5.181950e+14  7672.90
3201     3836.45  3805.00  3966.35  ...  1340273  5.185744e+14  7667.50

[3202 rows x 9 columns]
after scaling the data 
 [[ 0.38293873 -1.46335336 -1.49480655 ... 12.69005705  3.51856104
  -1.82092947]
 [-1.91394503 -1.8824744  -1.89009024 ...  4.93186656  0.82032375
  -1.82850628]
 [-1.93175152 -1.97916515 -1.91618812 ...  4.58713446  0.71382111
  -1.81870868]
 ...
 [ 2.9686407   3.0024404   3.16923577 ...  3.19226306  8.11491357
   2.86542511]
 [ 3.12866885  3.20632676  3.16969362 ...  3.48034075  8.65272306
   2.79579685]
 [ 3.0468511   2.99469594  3.18808424 ...  3.49720575  8.65993596
   2.79226971]]
2241
train 2146
x data after scaling 
[[ 0.38293873 -1.46335336 -1.49480655 ... 12.69005705  3.51856104
  -1.82092947]
 [-1.91394503 -1.8824744  -1.89009024 ...  4.93186656  0.82032375
  -1.82850628]
 [-1.93175152 -1.97916515 -1.91618812 ...  4.58713446  0.71382111
  -1.81870868]
 ...
 [ 1.57251974  1.56419426  1.55299859 ... -0.68655977 -0.42170836
   1.53086158]
 [ 1.56046966  1.56388755  1.52857952 ... -0.55939293 -0.22600525
   1.52779167]
 [ 1.55686231  1.53804712  1.5093495  ...  0.09224811  0.77308799
   1.4739702 ]]
len of sample data 9

2241
2241
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open   High  ...    VWAP   Volume      Turnover   Close
0  26-05-2008     2101.05  898.00  898.0  ...  624.61  3972485  2.481245e+14  604.75
1  27-05-2008      604.75  624.70  639.0  ...  606.43  1751063  1.061901e+14  593.15
2  28-05-2008      593.15  561.65  621.9  ...  608.75  1652355  1.005877e+14  608.15
3  29-05-2008      608.15  619.40  619.4  ...  600.98   669269  4.022170e+13  599.45
4  30-05-2008      599.45  605.40  607.0  ...  565.55  1262117  7.137940e+13  571.70

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover    Close
0        2101.05   898.00   898.00  ...  3972485  2.481245e+14   604.75
1         604.75   624.70   639.00  ...  1751063  1.061901e+14   593.15
2         593.15   561.65   621.90  ...  1652355  1.005877e+14   608.15
3         608.15   619.40   619.40  ...   669269  4.022170e+13   599.45
4         599.45   605.40   607.00  ...  1262117  7.137940e+13   571.70
...          ...      ...      ...  ...      ...           ...      ...
3197     3672.50  3682.20  3749.00  ...   480016  1.785560e+14  7477.70
3198     3738.85  3738.85  3798.00  ...   518487  1.952048e+14  7571.00
3199     3785.50  3810.05  3954.00  ...  1252958  4.899048e+14  7779.50
3200     3889.75  3943.00  3954.30  ...  1335444  5.181950e+14  7672.90
3201     3836.45  3805.00  3966.35  ...  1340273  5.185744e+14  7667.50

[3202 rows x 9 columns]
after scaling the data 
 [[ 0.38293873 -1.46335336 -1.49480655 ... 12.69005705  3.51856104
  -1.82092947]
 [-1.91394503 -1.8824744  -1.89009024 ...  4.93186656  0.82032375
  -1.82850628]
 [-1.93175152 -1.97916515 -1.91618812 ...  4.58713446  0.71382111
  -1.81870868]
 ...
 [ 2.9686407   3.0024404   3.16923577 ...  3.19226306  8.11491357
   2.86542511]
 [ 3.12866885  3.20632676  3.16969362 ...  3.48034075  8.65272306
   2.79579685]
 [ 3.0468511   2.99469594  3.18808424 ...  3.49720575  8.65993596
   2.79226971]]
369
val 274
x data after scaling 
[[ 1.46422251  1.46635334  1.46524256 ... -0.31981131  0.11263934
   1.42040999]
 [ 1.43068184  1.43276845  1.40640786 ...  0.24024738  0.95039123
   1.39069061]
 [ 1.39575963  1.43483875  1.43090324 ...  0.12118999  0.79105449
   1.45006406]
 ...
 [ 1.4546285   1.45347147  1.42891919 ...  0.5119944   1.36293051
   1.38866577]
 [ 1.39338031  1.34167518  1.41083382 ...  0.0342037   0.64357846
   1.42707236]
 [ 1.43851055  1.44810402  1.42136454 ...  0.54469764  1.40823085
   1.40695462]]
len of sample data 9

369
369
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open   High  ...    VWAP   Volume      Turnover   Close
0  26-05-2008     2101.05  898.00  898.0  ...  624.61  3972485  2.481245e+14  604.75
1  27-05-2008      604.75  624.70  639.0  ...  606.43  1751063  1.061901e+14  593.15
2  28-05-2008      593.15  561.65  621.9  ...  608.75  1652355  1.005877e+14  608.15
3  29-05-2008      608.15  619.40  619.4  ...  600.98   669269  4.022170e+13  599.45
4  30-05-2008      599.45  605.40  607.0  ...  565.55  1262117  7.137940e+13  571.70

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover    Close
0        2101.05   898.00   898.00  ...  3972485  2.481245e+14   604.75
1         604.75   624.70   639.00  ...  1751063  1.061901e+14   593.15
2         593.15   561.65   621.90  ...  1652355  1.005877e+14   608.15
3         608.15   619.40   619.40  ...   669269  4.022170e+13   599.45
4         599.45   605.40   607.00  ...  1262117  7.137940e+13   571.70
...          ...      ...      ...  ...      ...           ...      ...
3197     3672.50  3682.20  3749.00  ...   480016  1.785560e+14  7477.70
3198     3738.85  3738.85  3798.00  ...   518487  1.952048e+14  7571.00
3199     3785.50  3810.05  3954.00  ...  1252958  4.899048e+14  7779.50
3200     3889.75  3943.00  3954.30  ...  1335444  5.181950e+14  7672.90
3201     3836.45  3805.00  3966.35  ...  1340273  5.185744e+14  7667.50

[3202 rows x 9 columns]
after scaling the data 
 [[ 0.38293873 -1.46335336 -1.49480655 ... 12.69005705  3.51856104
  -1.82092947]
 [-1.91394503 -1.8824744  -1.89009024 ...  4.93186656  0.82032375
  -1.82850628]
 [-1.93175152 -1.97916515 -1.91618812 ...  4.58713446  0.71382111
  -1.81870868]
 ...
 [ 2.9686407   3.0024404   3.16923577 ...  3.19226306  8.11491357
   2.86542511]
 [ 3.12866885  3.20632676  3.16969362 ...  3.48034075  8.65272306
   2.79579685]
 [ 3.0468511   2.99469594  3.18808424 ...  3.49720575  8.65993596
   2.79226971]]
688
test 593
x data after scaling 
[[1.91920904 1.92442673 1.9054726  ... 0.78561686 2.12868852 1.8261612 ]
 [1.90746597 1.9196727  1.95873669 ... 1.11578896 2.72194015 1.84392751]
 [1.92834254 1.92550021 1.93363084 ... 1.4535222  3.27883336 1.8533332 ]
 ...
 [2.9686407  3.0024404  3.16923577 ... 3.19226306 8.11491357 2.86542511]
 [3.12866885 3.20632676 3.16969362 ... 3.48034075 8.65272306 2.79579685]
 [3.0468511  2.99469594 3.18808424 ... 3.49720575 8.65993596 2.79226971]]
len of sample data 9

2241
2241
2241
2241
Epoch: 1 cost time: 73.50363087654114
369
369
369
688
Epoch: 1, Steps: 67 | Train Loss: 0.2776902 Vali Loss: 0.2769642 Test Loss: 0.3694705
Validation loss decreased (inf --> 0.276964).  Saving model ...
Updating learning rate to 5.639613133480231e-05
2241
2241
2241
Epoch: 2 cost time: 78.33621907234192
369
369
369
688
Epoch: 2, Steps: 67 | Train Loss: 0.2055818 Vali Loss: 0.2288935 Test Loss: 0.3026329
Validation loss decreased (0.276964 --> 0.228894).  Saving model ...
Updating learning rate to 0.00010446438732775913
2241
2241
2241
Epoch: 3 cost time: 111.2554714679718
369
369
369
688
Epoch: 3, Steps: 67 | Train Loss: 0.1788865 Vali Loss: 0.2113184 Test Loss: 0.2735661
Validation loss decreased (0.228894 --> 0.211318).  Saving model ...
Updating learning rate to 0.00018092087865263936
2241
2241
2241
Epoch: 4 cost time: 70.36380982398987
369
369
369
688
Epoch: 4, Steps: 67 | Train Loss: 0.1626512 Vali Loss: 0.2029069 Test Loss: 0.2642885
Validation loss decreased (0.211318 --> 0.202907).  Saving model ...
Updating learning rate to 0.0002805423108379393
2241
2241
2241
Epoch: 5 cost time: 70.31858277320862
369
369
369
688
Epoch: 5, Steps: 67 | Train Loss: 0.1536430 Vali Loss: 0.2018823 Test Loss: 0.2509856
Validation loss decreased (0.202907 --> 0.201882).  Saving model ...
Updating learning rate to 0.00039652282519252863
2241
2241
2241
Epoch: 6 cost time: 70.43699145317078
369
369
369
688
Epoch: 6, Steps: 67 | Train Loss: 0.1494997 Vali Loss: 0.1964983 Test Loss: 0.2532838
Validation loss decreased (0.201882 --> 0.196498).  Saving model ...
Updating learning rate to 0.0005209389561095912
2241
2241
2241
Epoch: 7 cost time: 70.27295684814453
369
369
369
688
Epoch: 7, Steps: 67 | Train Loss: 0.1449662 Vali Loss: 0.1905246 Test Loss: 0.2550845
Validation loss decreased (0.196498 --> 0.190525).  Saving model ...
Updating learning rate to 0.000645290940161277
2241
2241
2241
Epoch: 8 cost time: 69.8054621219635
369
369
369
688
Epoch: 8, Steps: 67 | Train Loss: 0.1428604 Vali Loss: 0.1932592 Test Loss: 0.2508653
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.000761083396254858
2241
2241
2241
Epoch: 9 cost time: 70.01792478561401
369
369
369
688
Epoch: 9, Steps: 67 | Train Loss: 0.1403851 Vali Loss: 0.1964169 Test Loss: 0.2556270
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0008604057063999603
2241
2241
2241
Epoch: 10 cost time: 71.11417293548584
369
369
369
688
Epoch: 10, Steps: 67 | Train Loss: 0.1392254 Vali Loss: 0.1928476 Test Loss: 0.2549972
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0009364724470898548
2241
2241
2241
Epoch: 11 cost time: 70.06801986694336
369
369
369
688
Epoch: 11, Steps: 67 | Train Loss: 0.1380318 Vali Loss: 0.2036182 Test Loss: 0.2631292
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0009840869505305333
2241
2241
2241
Epoch: 12 cost time: 70.32109141349792
369
369
369
688
Epoch: 12, Steps: 67 | Train Loss: 0.1367256 Vali Loss: 0.2065214 Test Loss: 0.2529539
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0009999983035418297
2241
2241
2241
Epoch: 13 cost time: 70.3270616531372
369
369
369
688
Epoch: 13, Steps: 67 | Train Loss: 0.1349026 Vali Loss: 0.1988727 Test Loss: 0.2528103
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0009921760633032175
2241
2241
2241
Epoch: 14 cost time: 70.84655451774597
369
369
369
688
Epoch: 14, Steps: 67 | Train Loss: 0.1334907 Vali Loss: 0.1989316 Test Loss: 0.2654034
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00096939936320323
2241
2241
2241
Epoch: 15 cost time: 70.20386695861816
369
369
369
688
Epoch: 15, Steps: 67 | Train Loss: 0.1308801 Vali Loss: 0.2111283 Test Loss: 0.2703421
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0009323602617488389
2241
2241
2241
Epoch: 16 cost time: 69.72653460502625
369
369
369
688
Epoch: 16, Steps: 67 | Train Loss: 0.1282974 Vali Loss: 0.2114626 Test Loss: 0.2728063
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0008821841732950464
2241
2241
2241
Epoch: 17 cost time: 70.51082396507263
369
369
369
688
Epoch: 17, Steps: 67 | Train Loss: 0.1274079 Vali Loss: 0.2167669 Test Loss: 0.2703734
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : finance_bajajauto_48_48_PathFormer_BAJAJ-AUTO_ftMS_sl48_pl48_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open   High  ...    VWAP   Volume      Turnover   Close
0  26-05-2008     2101.05  898.00  898.0  ...  624.61  3972485  2.481245e+14  604.75
1  27-05-2008      604.75  624.70  639.0  ...  606.43  1751063  1.061901e+14  593.15
2  28-05-2008      593.15  561.65  621.9  ...  608.75  1652355  1.005877e+14  608.15
3  29-05-2008      608.15  619.40  619.4  ...  600.98   669269  4.022170e+13  599.45
4  30-05-2008      599.45  605.40  607.0  ...  565.55  1262117  7.137940e+13  571.70

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover    Close
0        2101.05   898.00   898.00  ...  3972485  2.481245e+14   604.75
1         604.75   624.70   639.00  ...  1751063  1.061901e+14   593.15
2         593.15   561.65   621.90  ...  1652355  1.005877e+14   608.15
3         608.15   619.40   619.40  ...   669269  4.022170e+13   599.45
4         599.45   605.40   607.00  ...  1262117  7.137940e+13   571.70
...          ...      ...      ...  ...      ...           ...      ...
3197     3672.50  3682.20  3749.00  ...   480016  1.785560e+14  7477.70
3198     3738.85  3738.85  3798.00  ...   518487  1.952048e+14  7571.00
3199     3785.50  3810.05  3954.00  ...  1252958  4.899048e+14  7779.50
3200     3889.75  3943.00  3954.30  ...  1335444  5.181950e+14  7672.90
3201     3836.45  3805.00  3966.35  ...  1340273  5.185744e+14  7667.50

[3202 rows x 9 columns]
after scaling the data 
 [[ 0.38293873 -1.46335336 -1.49480655 ... 12.69005705  3.51856104
  -1.82092947]
 [-1.91394503 -1.8824744  -1.89009024 ...  4.93186656  0.82032375
  -1.82850628]
 [-1.93175152 -1.97916515 -1.91618812 ...  4.58713446  0.71382111
  -1.81870868]
 ...
 [ 2.9686407   3.0024404   3.16923577 ...  3.19226306  8.11491357
   2.86542511]
 [ 3.12866885  3.20632676  3.16969362 ...  3.48034075  8.65272306
   2.79579685]
 [ 3.0468511   2.99469594  3.18808424 ...  3.49720575  8.65993596
   2.79226971]]
688
test 593
x data after scaling 
[[1.91920904 1.92442673 1.9054726  ... 0.78561686 2.12868852 1.8261612 ]
 [1.90746597 1.9196727  1.95873669 ... 1.11578896 2.72194015 1.84392751]
 [1.92834254 1.92550021 1.93363084 ... 1.4535222  3.27883336 1.8533332 ]
 ...
 [2.9686407  3.0024404  3.16923577 ... 3.19226306 8.11491357 2.86542511]
 [3.12866885 3.20632676 3.16969362 ... 3.48034075 8.65272306 2.79579685]
 [3.0468511  2.99469594 3.18808424 ... 3.49720575 8.65993596 2.79226971]]
len of sample data 9

688
(576, 48, 1)
(576, 48, 1)
1.9136272370815277
mse:20061.228515625, mae:107.20075225830078, rse:0.25027996301651 ,mape : 1.9136272370815277
Inference time:  36.49578642845154
