Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_britannia_48_48', data='custom', root_path='./dataset/finance', data_path='BRITANNIA.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_britannia_48_48_PathFormer_BRITANNIA_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open  ...  Volume      Turnover     Close
0  03-01-2000      703.25  705.0  ...    7512  5.566490e+11  1135.350
1  04-01-2000      756.90  710.0  ...    8135  6.040390e+11  1131.825
2  05-01-2000      754.55  755.0  ...    6095  4.509780e+11  1102.950
3  06-01-2000      735.30  740.0  ...   19697  1.553760e+12  1178.475
4  07-01-2000      785.65  808.0  ...   33107  2.739710e+12  1272.750

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover      Close
0         703.25   705.00   759.50  ...     7512  5.566490e+11   1135.350
1         756.90   710.00   770.00  ...     8135  6.040390e+11   1131.825
2         754.55   755.00   759.00  ...     6095  4.509780e+11   1102.950
3         735.30   740.00   794.15  ...    19697  1.553760e+12   1178.475
4         785.65   808.00   848.50  ...    33107  2.739710e+12   1272.750
...          ...      ...      ...  ...      ...           ...        ...
5300     3650.30  3661.10  3662.00  ...  1031406  3.670380e+14  53088.000
5301     3539.20  3572.70  3574.60  ...   813608  2.879700e+14  53118.000
5302     3541.20  3488.00  3527.00  ...  1498181  5.236340e+14  52181.250
5303     3478.75  3496.75  3505.90  ...   611087  2.121190e+14  52033.500
5304     3468.90  3460.00  3468.35  ...   436187  1.505070e+14  51735.000

[5305 rows x 9 columns]
after scaling the data 
 [[-0.47894476 -0.47544    -0.38120385 ... -0.30115553 -0.30589917
  -0.68794736]
 [-0.35663113 -0.46407974 -0.3577853  ... -0.29337424 -0.29821926
  -0.68979466]
 [-0.36198877 -0.36183741 -0.38231902 ... -0.3188539  -0.32302395
  -0.70492683]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
3713
train 3618
x data after scaling 
[[-0.47894476 -0.47544    -0.38120385 ... -0.30115553 -0.30589917
  -0.68794736]
 [-0.35663113 -0.46407974 -0.3577853  ... -0.29337424 -0.29821926
  -0.68979466]
 [-0.36198877 -0.36183741 -0.38231902 ... -0.3188539  -0.32302395
  -0.70492683]
 ...
 [ 1.7909818   1.80342797  1.75590625 ...  0.52534729  1.61711636
   5.29071266]
 [ 1.73079392  1.74651307  1.68252812 ...  0.90013564  2.42250995
   5.29582222]
 [ 1.73375772  1.74889872  1.70527757 ...  0.53701297  1.62244805
   5.21230052]]
len of sample data 9

3713
3713
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open  ...  Volume      Turnover     Close
0  03-01-2000      703.25  705.0  ...    7512  5.566490e+11  1135.350
1  04-01-2000      756.90  710.0  ...    8135  6.040390e+11  1131.825
2  05-01-2000      754.55  755.0  ...    6095  4.509780e+11  1102.950
3  06-01-2000      735.30  740.0  ...   19697  1.553760e+12  1178.475
4  07-01-2000      785.65  808.0  ...   33107  2.739710e+12  1272.750

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover      Close
0         703.25   705.00   759.50  ...     7512  5.566490e+11   1135.350
1         756.90   710.00   770.00  ...     8135  6.040390e+11   1131.825
2         754.55   755.00   759.00  ...     6095  4.509780e+11   1102.950
3         735.30   740.00   794.15  ...    19697  1.553760e+12   1178.475
4         785.65   808.00   848.50  ...    33107  2.739710e+12   1272.750
...          ...      ...      ...  ...      ...           ...        ...
5300     3650.30  3661.10  3662.00  ...  1031406  3.670380e+14  53088.000
5301     3539.20  3572.70  3574.60  ...   813608  2.879700e+14  53118.000
5302     3541.20  3488.00  3527.00  ...  1498181  5.236340e+14  52181.250
5303     3478.75  3496.75  3505.90  ...   611087  2.121190e+14  52033.500
5304     3468.90  3460.00  3468.35  ...   436187  1.505070e+14  51735.000

[5305 rows x 9 columns]
after scaling the data 
 [[-0.47894476 -0.47544    -0.38120385 ... -0.30115553 -0.30589917
  -0.68794736]
 [-0.35663113 -0.46407974 -0.3577853  ... -0.29337424 -0.29821926
  -0.68979466]
 [-0.36198877 -0.36183741 -0.38231902 ... -0.3188539  -0.32302395
  -0.70492683]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
579
val 484
x data after scaling 
[[ 0.8155506   0.84166844  0.82429397 ...  2.00967504  3.61450804
   3.72856212]
 [ 0.82466997  0.83098979  0.85763753 ...  3.81129955  6.72068124
   3.85040554]
 [ 0.89534513  0.89915135  0.9913463  ...  2.43261247  4.54943423
   4.00703329]
 ...
 [ 4.48552928  4.47763295  4.5824139  ...  3.92483393 15.97915855
  10.39084232]
 [ 4.68911934  4.70472452  4.60460577 ...  1.22936037  5.84315583
  10.30319367]
 [ 4.63827882  4.6684853   4.5824139  ...  0.90809179  4.59727362
  10.27253629]]
len of sample data 9

579
579
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open  ...  Volume      Turnover     Close
0  03-01-2000      703.25  705.0  ...    7512  5.566490e+11  1135.350
1  04-01-2000      756.90  710.0  ...    8135  6.040390e+11  1131.825
2  05-01-2000      754.55  755.0  ...    6095  4.509780e+11  1102.950
3  06-01-2000      735.30  740.0  ...   19697  1.553760e+12  1178.475
4  07-01-2000      785.65  808.0  ...   33107  2.739710e+12  1272.750

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover      Close
0         703.25   705.00   759.50  ...     7512  5.566490e+11   1135.350
1         756.90   710.00   770.00  ...     8135  6.040390e+11   1131.825
2         754.55   755.00   759.00  ...     6095  4.509780e+11   1102.950
3         735.30   740.00   794.15  ...    19697  1.553760e+12   1178.475
4         785.65   808.00   848.50  ...    33107  2.739710e+12   1272.750
...          ...      ...      ...  ...      ...           ...        ...
5300     3650.30  3661.10  3662.00  ...  1031406  3.670380e+14  53088.000
5301     3539.20  3572.70  3574.60  ...   813608  2.879700e+14  53118.000
5302     3541.20  3488.00  3527.00  ...  1498181  5.236340e+14  52181.250
5303     3478.75  3496.75  3505.90  ...   611087  2.121190e+14  52033.500
5304     3468.90  3460.00  3468.35  ...   436187  1.505070e+14  51735.000

[5305 rows x 9 columns]
after scaling the data 
 [[-0.47894476 -0.47544    -0.38120385 ... -0.30115553 -0.30589917
  -0.68794736]
 [-0.35663113 -0.46407974 -0.3577853  ... -0.29337424 -0.29821926
  -0.68979466]
 [-0.36198877 -0.36183741 -0.38231902 ... -0.3188539  -0.32302395
  -0.70492683]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
1109
test 1014
x data after scaling 
[[ 5.44705344  5.37054931  5.28876202 ...  5.26772457 23.42959221
  11.43417572]
 [ 5.2943039   4.96612408  5.04342482 ...  2.42576793 10.99195098
  10.99652202]
 [ 5.04044329  5.22741004  5.15527628 ...  1.31957586  6.73008058
  11.31076013]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
len of sample data 9

3713
3713
3713
3713
	iters: 100, epoch: 1 | loss: 0.1347612
	speed: 1.4514s/iter; left time: 3136.5609s
Epoch: 1 cost time: 158.25648760795593
579
579
579
1109
Epoch: 1, Steps: 113 | Train Loss: 0.1556610 Vali Loss: 0.9935462 Test Loss: 1.7161247
Validation loss decreased (inf --> 0.993546).  Saving model ...
Updating learning rate to 7.661774908130671e-05
3713
3713
3713
	iters: 100, epoch: 2 | loss: 0.1307083
	speed: 2.1669s/iter; left time: 4437.8290s
Epoch: 2 cost time: 142.71594786643982
579
579
579
1109
Epoch: 2, Steps: 113 | Train Loss: 0.1138493 Vali Loss: 0.8358943 Test Loss: 1.4605436
Validation loss decreased (0.993546 --> 0.835894).  Saving model ...
Updating learning rate to 0.00018088408154280373
3713
3713
3713
	iters: 100, epoch: 3 | loss: 0.1088978
	speed: 2.2102s/iter; left time: 4276.7477s
Epoch: 3 cost time: 149.66449642181396
579
579
579
1109
Epoch: 3, Steps: 113 | Train Loss: 0.0994244 Vali Loss: 0.7960034 Test Loss: 1.3489263
Validation loss decreased (0.835894 --> 0.796003).  Saving model ...
Updating learning rate to 0.00033689067071906087
3713
3713
3713
	iters: 100, epoch: 4 | loss: 0.0883970
	speed: 2.1559s/iter; left time: 3928.0548s
Epoch: 4 cost time: 120.08822560310364
579
579
579
1109
Epoch: 4, Steps: 113 | Train Loss: 0.0932643 Vali Loss: 0.7802210 Test Loss: 1.3166338
Validation loss decreased (0.796003 --> 0.780221).  Saving model ...
Updating learning rate to 0.0005208349743705507
3713
3713
3713
	iters: 100, epoch: 5 | loss: 0.0684619
	speed: 2.3446s/iter; left time: 4006.9435s
Epoch: 5 cost time: 126.86969828605652
579
579
579
1109
Epoch: 5, Steps: 113 | Train Loss: 0.0899567 Vali Loss: 0.7645199 Test Loss: 1.2935021
Validation loss decreased (0.780221 --> 0.764520).  Saving model ...
Updating learning rate to 0.0007046518826804148
3713
3713
3713
	iters: 100, epoch: 6 | loss: 0.0905808
	speed: 2.5066s/iter; left time: 4000.5933s
Epoch: 6 cost time: 124.46351671218872
579
579
579
1109
Epoch: 6, Steps: 113 | Train Loss: 0.0881902 Vali Loss: 0.7532839 Test Loss: 1.3515681
Validation loss decreased (0.764520 --> 0.753284).  Saving model ...
Updating learning rate to 0.0008602957230428527
3713
3713
3713
	iters: 100, epoch: 7 | loss: 0.0999979
	speed: 2.2121s/iter; left time: 3280.4838s
Epoch: 7 cost time: 109.98491930961609
579
579
579
1109
Epoch: 7, Steps: 113 | Train Loss: 0.0873409 Vali Loss: 0.7545194 Test Loss: 1.2928567
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0009640192992393529
3713
3713
3713
	iters: 100, epoch: 8 | loss: 0.1100862
	speed: 2.1952s/iter; left time: 3007.4861s
Epoch: 8 cost time: 119.27829551696777
579
579
579
1109
Epoch: 8, Steps: 113 | Train Loss: 0.0863009 Vali Loss: 0.7608046 Test Loss: 1.2841915
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0009999986581052816
3713
3713
3713
	iters: 100, epoch: 9 | loss: 0.0754966
	speed: 2.1331s/iter; left time: 2681.3481s
Epoch: 9 cost time: 109.63774490356445
579
579
579
1109
Epoch: 9, Steps: 113 | Train Loss: 0.0858379 Vali Loss: 0.7468049 Test Loss: 1.3055724
Validation loss decreased (0.753284 --> 0.746805).  Saving model ...
Updating learning rate to 0.0009826618694765096
3713
3713
3713
	iters: 100, epoch: 10 | loss: 0.0912160
	speed: 2.1255s/iter; left time: 2431.5369s
Epoch: 10 cost time: 111.52468132972717
579
579
579
1109
Epoch: 10, Steps: 113 | Train Loss: 0.0852976 Vali Loss: 0.7489424 Test Loss: 1.2857630
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0009324326083760611
3713
3713
3713
	iters: 100, epoch: 11 | loss: 0.0760574
	speed: 2.5003s/iter; left time: 2577.8549s
Epoch: 11 cost time: 140.67276191711426
579
579
579
1109
Epoch: 11, Steps: 113 | Train Loss: 0.0846262 Vali Loss: 0.7376067 Test Loss: 1.3041011
Validation loss decreased (0.746805 --> 0.737607).  Saving model ...
Updating learning rate to 0.000852733915940153
3713
3713
3713
	iters: 100, epoch: 12 | loss: 0.0754569
	speed: 2.5592s/iter; left time: 2349.3687s
Epoch: 12 cost time: 136.09145331382751
579
579
579
1109
Epoch: 12, Steps: 113 | Train Loss: 0.0840098 Vali Loss: 0.7701828 Test Loss: 1.3054978
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0007489971263499759
3713
3713
3713
	iters: 100, epoch: 13 | loss: 0.0771151
	speed: 2.5323s/iter; left time: 2038.4858s
Epoch: 13 cost time: 133.44736909866333
579
579
579
1109
Epoch: 13, Steps: 113 | Train Loss: 0.0839398 Vali Loss: 0.7439664 Test Loss: 1.2830069
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0006282917303829499
3713
3713
3713
	iters: 100, epoch: 14 | loss: 0.0910452
	speed: 2.4896s/iter; left time: 1722.8345s
Epoch: 14 cost time: 132.10079789161682
579
579
579
1109
Epoch: 14, Steps: 113 | Train Loss: 0.0835725 Vali Loss: 0.7434665 Test Loss: 1.3073307
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0004988436012991294
3713
3713
3713
	iters: 100, epoch: 15 | loss: 0.0910726
	speed: 2.5394s/iter; left time: 1470.3413s
Epoch: 15 cost time: 136.5955250263214
579
579
579
1109
Epoch: 15, Steps: 113 | Train Loss: 0.0825135 Vali Loss: 0.7406657 Test Loss: 1.2991269
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00036947441517242894
3713
3713
3713
	iters: 100, epoch: 16 | loss: 0.1002505
	speed: 3.7322s/iter; left time: 1739.2236s
Epoch: 16 cost time: 251.19538378715515
579
579
579
1109
Epoch: 16, Steps: 113 | Train Loss: 0.0821128 Vali Loss: 0.7473704 Test Loss: 1.3085067
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002490004682446946
3713
3713
3713
	iters: 100, epoch: 17 | loss: 0.0542044
	speed: 4.2591s/iter; left time: 1503.4678s
Epoch: 17 cost time: 266.2546808719635
579
579
579
1109
Epoch: 17, Steps: 113 | Train Loss: 0.0813659 Vali Loss: 0.7473549 Test Loss: 1.3055422
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00014563186090644047
3713
3713
3713
	iters: 100, epoch: 18 | loss: 0.0942154
	speed: 5.1822s/iter; left time: 1243.7330s
Epoch: 18 cost time: 299.33186388015747
579
579
579
1109
Epoch: 18, Steps: 113 | Train Loss: 0.0812968 Vali Loss: 0.7450402 Test Loss: 1.3102610
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.641299292306835e-05
3713
3713
3713
	iters: 100, epoch: 19 | loss: 0.0611026
	speed: 4.9839s/iter; left time: 632.9615s
Epoch: 19 cost time: 291.6137866973877
579
579
579
1109
Epoch: 19, Steps: 113 | Train Loss: 0.0808396 Vali Loss: 0.7442796 Test Loss: 1.3052508
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.6742499232275877e-05
3713
3713
3713
	iters: 100, epoch: 20 | loss: 0.0869505
	speed: 2.8298s/iter; left time: 39.6165s
Epoch: 20 cost time: 158.55427765846252
579
579
579
1109
Epoch: 20, Steps: 113 | Train Loss: 0.0808295 Vali Loss: 0.7446260 Test Loss: 1.3027589
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.341894718448942e-09
>>>>>>>testing : finance_britannia_48_48_PathFormer_BRITANNIA_ftMS_sl48_pl48_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close   Open  ...  Volume      Turnover     Close
0  03-01-2000      703.25  705.0  ...    7512  5.566490e+11  1135.350
1  04-01-2000      756.90  710.0  ...    8135  6.040390e+11  1131.825
2  05-01-2000      754.55  755.0  ...    6095  4.509780e+11  1102.950
3  06-01-2000      735.30  740.0  ...   19697  1.553760e+12  1178.475
4  07-01-2000      785.65  808.0  ...   33107  2.739710e+12  1272.750

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...   Volume      Turnover      Close
0         703.25   705.00   759.50  ...     7512  5.566490e+11   1135.350
1         756.90   710.00   770.00  ...     8135  6.040390e+11   1131.825
2         754.55   755.00   759.00  ...     6095  4.509780e+11   1102.950
3         735.30   740.00   794.15  ...    19697  1.553760e+12   1178.475
4         785.65   808.00   848.50  ...    33107  2.739710e+12   1272.750
...          ...      ...      ...  ...      ...           ...        ...
5300     3650.30  3661.10  3662.00  ...  1031406  3.670380e+14  53088.000
5301     3539.20  3572.70  3574.60  ...   813608  2.879700e+14  53118.000
5302     3541.20  3488.00  3527.00  ...  1498181  5.236340e+14  52181.250
5303     3478.75  3496.75  3505.90  ...   611087  2.121190e+14  52033.500
5304     3468.90  3460.00  3468.35  ...   436187  1.505070e+14  51735.000

[5305 rows x 9 columns]
after scaling the data 
 [[-0.47894476 -0.47544    -0.38120385 ... -0.30115553 -0.30589917
  -0.68794736]
 [-0.35663113 -0.46407974 -0.3577853  ... -0.29337424 -0.29821926
  -0.68979466]
 [-0.36198877 -0.36183741 -0.38231902 ... -0.3188539  -0.32302395
  -0.70492683]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
1109
test 1014
x data after scaling 
[[ 5.44705344  5.37054931  5.28876202 ...  5.26772457 23.42959221
  11.43417572]
 [ 5.2943039   4.96612408  5.04342482 ...  2.42576793 10.99195098
  10.99652202]
 [ 5.04044329  5.22741004  5.15527628 ...  1.31957586  6.73008058
  11.31076013]
 ...
 [ 5.99113818  5.84768019  5.79125723 ... 18.31734629 84.46273321
  26.06305026]
 [ 5.84876193  5.86756064  5.74419709 ...  7.23751485 33.97937646
  25.98562073]
 [ 5.82630547  5.78406274  5.66044789 ...  5.05300845 23.99468688
  25.82918951]]
len of sample data 9

1109
(992, 48, 1)
(992, 48, 1)
1.1908951215445995
mse:561882.9375, mae:502.2023620605469, rse:0.08278550207614899 ,mape : 1.1908951215445995
Inference time:  60.355347871780396
