Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_cipla_48_48', data='custom', root_path='./dataset/finance', data_path=' CIPLA.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_cipla_48_48_PathFormer_ CIPLA_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Args in experiment:
Namespace(is_training=1, model='PathFormer', model_id='finance_cipla_48_48', data='custom', root_path='./dataset/finance', data_path='CIPLA.csv', features='MS', target='Close', freq='d', checkpoints='./checkpoints/', seq_len=48, pred_len=48, individual=False, d_model=8, d_ff=64, num_nodes=9, layer_nums=3, k=2, num_experts_list=[4, 4, 4], patch_size_list=[[16, 12, 8, 4], [12, 8, 6, 4], [8, 6, 2, 12]], do_predict=False, revin=1, drop=0.1, embed='timeF', residual_connection=1, metric='mae', batch_norm=0, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=10, learning_rate=0.001, lradj='TST', use_amp=False, pct_start=0.4, use_gpu=False, gpu=0, use_multi_gpu=False, devices='2', test_flop=False)
Use CPU
>>>>>>>start training : finance_cipla_48_48_PathFormer_CIPLA_ftMS_sl48_pl48_0>>>>>>>>>>>>>>>>>>>>>>>>>>
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...  Volume      Turnover    Close
0  03-01-2000     1349.40  1410.0  ...   21060  3.035500e+12  4372.05
1  04-01-2000     1457.35  1537.0  ...   30215  4.412700e+12  4395.75
2  05-01-2000     1465.25  1474.0  ...   33799  4.826870e+12  4305.15
3  06-01-2000     1435.05  1434.0  ...   33083  4.600360e+12  4067.55
4  07-01-2000     1355.85  1370.0  ...   66536  8.433350e+12  3742.65

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...    Volume      Turnover      Close
0        1349.40  1410.00  1457.35  ...     21060  3.035500e+12   4372.050
1        1457.35  1537.00  1537.00  ...     30215  4.412700e+12   4395.750
2        1465.25  1474.00  1474.00  ...     33799  4.826870e+12   4305.150
3        1435.05  1434.00  1435.00  ...     33083  4.600360e+12   4067.550
4        1355.85  1370.00  1389.90  ...     66536  8.433350e+12   3742.650
...          ...      ...      ...  ...       ...           ...        ...
5301      935.60   935.60   940.00  ...  10255697  9.374880e+14  33952.500
5302      905.40   913.00   919.50  ...   5669049  5.154420e+14  34215.000
5303      912.40   914.35   918.00  ...   7251009  6.597970e+14  34132.500
5304      910.20   911.95   917.40  ...   4953091  4.508420e+14  33993.750
5305      906.50   900.75   921.00  ...   6459737  5.887820e+14  34138.125

[5306 rows x 9 columns]
after scaling the data 
 [[ 2.47872585  2.65155099  2.71669996 ... -0.90224236 -0.81706707
  -0.79786747]
 [ 2.7993685   3.02807816  2.94856278 ... -0.89342097 -0.77846687
  -0.7928865 ]
 [ 2.82283377  2.84129696  2.76516846 ... -0.88996757 -0.7668585
  -0.81192768]
 ...
 [ 1.18070979  1.18205734  1.14664081 ...  6.06424465 17.59066244
   5.4568124 ]
 [ 1.17417515  1.17494187  1.1448942  ...  3.85006363 11.73406592
   5.42765166]
 [ 1.16318508  1.14173632  1.15537387 ...  5.30180689 15.60025206
   5.45799459]]
3714
train 3619
x data after scaling 
[[ 2.47872585  2.65155099  2.71669996 ... -0.90224236 -0.81706707
  -0.79786747]
 [ 2.7993685   3.02807816  2.94856278 ... -0.89342097 -0.77846687
  -0.7928865 ]
 [ 2.82283377  2.84129696  2.76516846 ... -0.88996757 -0.7668585
  -0.81192768]
 ...
 [ 0.25041543  0.25007847  0.2906551  ...  1.25654748  2.97626021
   3.15784248]
 [ 0.307742    0.32849692  0.35193209 ...  1.8215432   4.16282676
   3.22522744]
 [ 0.33313797  0.33012755  0.31408882 ...  0.71863357  2.06860056
   3.15508403]]
len of sample data 9

3714
3714
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...  Volume      Turnover    Close
0  03-01-2000     1349.40  1410.0  ...   21060  3.035500e+12  4372.05
1  04-01-2000     1457.35  1537.0  ...   30215  4.412700e+12  4395.75
2  05-01-2000     1465.25  1474.0  ...   33799  4.826870e+12  4305.15
3  06-01-2000     1435.05  1434.0  ...   33083  4.600360e+12  4067.55
4  07-01-2000     1355.85  1370.0  ...   66536  8.433350e+12  3742.65

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...    Volume      Turnover      Close
0        1349.40  1410.00  1457.35  ...     21060  3.035500e+12   4372.050
1        1457.35  1537.00  1537.00  ...     30215  4.412700e+12   4395.750
2        1465.25  1474.00  1474.00  ...     33799  4.826870e+12   4305.150
3        1435.05  1434.00  1435.00  ...     33083  4.600360e+12   4067.550
4        1355.85  1370.00  1389.90  ...     66536  8.433350e+12   3742.650
...          ...      ...      ...  ...       ...           ...        ...
5301      935.60   935.60   940.00  ...  10255697  9.374880e+14  33952.500
5302      905.40   913.00   919.50  ...   5669049  5.154420e+14  34215.000
5303      912.40   914.35   918.00  ...   7251009  6.597970e+14  34132.500
5304      910.20   911.95   917.40  ...   4953091  4.508420e+14  33993.750
5305      906.50   900.75   921.00  ...   6459737  5.887820e+14  34138.125

[5306 rows x 9 columns]
after scaling the data 
 [[ 2.47872585  2.65155099  2.71669996 ... -0.90224236 -0.81706707
  -0.79786747]
 [ 2.7993685   3.02807816  2.94856278 ... -0.89342097 -0.77846687
  -0.7928865 ]
 [ 2.82283377  2.84129696  2.76516846 ... -0.88996757 -0.7668585
  -0.81192768]
 ...
 [ 1.18070979  1.18205734  1.14664081 ...  6.06424465 17.59066244
   5.4568124 ]
 [ 1.17417515  1.17494187  1.1448942  ...  3.85006363 11.73406592
   5.42765166]
 [ 1.16318508  1.14173632  1.15537387 ...  5.30180689 15.60025206
   5.45799459]]
579
val 484
x data after scaling 
[[ 0.14422762  0.15253718  0.15718479 ...  0.8925214   2.12154551
   2.81421859]
 [ 0.17823742  0.18485329  0.16970218 ...  0.47524901  1.4498314
   2.83352773]
 [ 0.18551463  0.18188851  0.16096911 ...  0.37534714  1.25923801
   2.81658297]
 ...
 [ 0.17823742  0.18174027  0.1598047  ... -0.39864177 -0.02665987
   2.83037521]
 [ 0.18432651  0.17284593  0.18542169 ...  0.12394769  0.86518917
   2.90367114]
 [ 0.2119502   0.21153632  0.18920602 ... -0.38580908  0.0102138
   2.88120948]]
len of sample data 9

579
579
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...  Volume      Turnover    Close
0  03-01-2000     1349.40  1410.0  ...   21060  3.035500e+12  4372.05
1  04-01-2000     1457.35  1537.0  ...   30215  4.412700e+12  4395.75
2  05-01-2000     1465.25  1474.0  ...   33799  4.826870e+12  4305.15
3  06-01-2000     1435.05  1434.0  ...   33083  4.600360e+12  4067.55
4  07-01-2000     1355.85  1370.0  ...   66536  8.433350e+12  3742.65

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...    Volume      Turnover      Close
0        1349.40  1410.00  1457.35  ...     21060  3.035500e+12   4372.050
1        1457.35  1537.00  1537.00  ...     30215  4.412700e+12   4395.750
2        1465.25  1474.00  1474.00  ...     33799  4.826870e+12   4305.150
3        1435.05  1434.00  1435.00  ...     33083  4.600360e+12   4067.550
4        1355.85  1370.00  1389.90  ...     66536  8.433350e+12   3742.650
...          ...      ...      ...  ...       ...           ...        ...
5301      935.60   935.60   940.00  ...  10255697  9.374880e+14  33952.500
5302      905.40   913.00   919.50  ...   5669049  5.154420e+14  34215.000
5303      912.40   914.35   918.00  ...   7251009  6.597970e+14  34132.500
5304      910.20   911.95   917.40  ...   4953091  4.508420e+14  33993.750
5305      906.50   900.75   921.00  ...   6459737  5.887820e+14  34138.125

[5306 rows x 9 columns]
after scaling the data 
 [[ 2.47872585  2.65155099  2.71669996 ... -0.90224236 -0.81706707
  -0.79786747]
 [ 2.7993685   3.02807816  2.94856278 ... -0.89342097 -0.77846687
  -0.7928865 ]
 [ 2.82283377  2.84129696  2.76516846 ... -0.88996757 -0.7668585
  -0.81192768]
 ...
 [ 1.18070979  1.18205734  1.14664081 ...  6.06424465 17.59066244
   5.4568124 ]
 [ 1.17417515  1.17494187  1.1448942  ...  3.85006363 11.73406592
   5.42765166]
 [ 1.16318508  1.14173632  1.15537387 ...  5.30180689 15.60025206
   5.45799459]]
1109
test 1014
x data after scaling 
[[ 9.16535136e-02  1.00949991e-01  7.80050156e-02 ...  5.59964999e-02
   6.26802382e-01  2.50211982e+00]
 [ 6.06139989e-02 -1.05257704e-02  2.29867195e-02 ...  2.74955037e+00
   4.68126119e+00  2.43118829e+00]
 [ 3.38814025e-02  8.70155207e-02  1.27783474e-01 ...  2.91511786e+00
   5.34423616e+00  2.72594823e+00]
 ...
 [ 1.18070979e+00  1.18205734e+00  1.14664081e+00 ...  6.06424465e+00
   1.75906624e+01  5.45681240e+00]
 [ 1.17417515e+00  1.17494187e+00  1.14489420e+00 ...  3.85006363e+00
   1.17340659e+01  5.42765166e+00]
 [ 1.16318508e+00  1.14173632e+00  1.15537387e+00 ...  5.30180689e+00
   1.56002521e+01  5.45799459e+00]]
len of sample data 9

3714
3714
3714
3714
	iters: 100, epoch: 1 | loss: 0.2039121
	speed: 0.8481s/iter; left time: 1832.6403s
Epoch: 1 cost time: 101.31809544563293
579
579
579
1109
Epoch: 1, Steps: 113 | Train Loss: 0.2271818 Vali Loss: 0.3975892 Test Loss: 0.3653030
Validation loss decreased (inf --> 0.397589).  Saving model ...
Updating learning rate to 7.661774908130671e-05
3714
3714
3714
	iters: 100, epoch: 2 | loss: 0.1750850
	speed: 2.4080s/iter; left time: 4931.6150s
Epoch: 2 cost time: 146.7502999305725
579
579
579
1109
Epoch: 2, Steps: 113 | Train Loss: 0.1666979 Vali Loss: 0.3255762 Test Loss: 0.3044887
Validation loss decreased (0.397589 --> 0.325576).  Saving model ...
Updating learning rate to 0.00018088408154280373
3714
3714
3714
	iters: 100, epoch: 3 | loss: 0.1352162
	speed: 2.1643s/iter; left time: 4187.8557s
Epoch: 3 cost time: 146.12419056892395
579
579
579
1109
Epoch: 3, Steps: 113 | Train Loss: 0.1434878 Vali Loss: 0.3134253 Test Loss: 0.2803183
Validation loss decreased (0.325576 --> 0.313425).  Saving model ...
Updating learning rate to 0.00033689067071906087
3714
3714
3714
	iters: 100, epoch: 4 | loss: 0.1448562
	speed: 1.9774s/iter; left time: 3602.7653s
Epoch: 4 cost time: 121.17488718032837
579
579
579
1109
Epoch: 4, Steps: 113 | Train Loss: 0.1322136 Vali Loss: 0.3173259 Test Loss: 0.2757687
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0005208349743705507
3714
3714
3714
	iters: 100, epoch: 5 | loss: 0.1406451
	speed: 2.2750s/iter; left time: 3887.9599s
Epoch: 5 cost time: 126.4713032245636
579
579
579
1109
Epoch: 5, Steps: 113 | Train Loss: 0.1258273 Vali Loss: 0.3017572 Test Loss: 0.2682691
Validation loss decreased (0.313425 --> 0.301757).  Saving model ...
Updating learning rate to 0.0007046518826804148
3714
3714
3714
	iters: 100, epoch: 6 | loss: 0.1392295
	speed: 2.5145s/iter; left time: 4013.0725s
Epoch: 6 cost time: 147.1215043067932
579
579
579
1109
Epoch: 6, Steps: 113 | Train Loss: 0.1223603 Vali Loss: 0.2973736 Test Loss: 0.2649931
Validation loss decreased (0.301757 --> 0.297374).  Saving model ...
Updating learning rate to 0.0008602957230428527
3714
3714
3714
	iters: 100, epoch: 7 | loss: 0.1403141
	speed: 2.4077s/iter; left time: 3570.6635s
Epoch: 7 cost time: 130.43569254875183
579
579
579
1109
Epoch: 7, Steps: 113 | Train Loss: 0.1200596 Vali Loss: 0.3022005 Test Loss: 0.2682985
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0009640192992393529
3714
3714
3714
	iters: 100, epoch: 8 | loss: 0.1116378
	speed: 2.0983s/iter; left time: 2874.7095s
Epoch: 8 cost time: 117.8597183227539
579
579
579
1109
Epoch: 8, Steps: 113 | Train Loss: 0.1191569 Vali Loss: 0.2981805 Test Loss: 0.2645137
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0009999986581052816
3714
3714
3714
	iters: 100, epoch: 9 | loss: 0.1387640
	speed: 2.2293s/iter; left time: 2802.1959s
Epoch: 9 cost time: 121.14742302894592
579
579
579
1109
Epoch: 9, Steps: 113 | Train Loss: 0.1169750 Vali Loss: 0.2844579 Test Loss: 0.2668655
Validation loss decreased (0.297374 --> 0.284458).  Saving model ...
Updating learning rate to 0.0009826618694765096
3714
3714
3714
	iters: 100, epoch: 10 | loss: 0.1328867
	speed: 2.1003s/iter; left time: 2402.7442s
Epoch: 10 cost time: 117.60901165008545
579
579
579
1109
Epoch: 10, Steps: 113 | Train Loss: 0.1166305 Vali Loss: 0.2952326 Test Loss: 0.2639500
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0009324326083760611
3714
3714
3714
	iters: 100, epoch: 11 | loss: 0.1109051
	speed: 2.2100s/iter; left time: 2278.5431s
Epoch: 11 cost time: 127.67979025840759
579
579
579
1109
Epoch: 11, Steps: 113 | Train Loss: 0.1157599 Vali Loss: 0.3011882 Test Loss: 0.2672465
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.000852733915940153
3714
3714
3714
	iters: 100, epoch: 12 | loss: 0.1219585
	speed: 2.5949s/iter; left time: 2382.0877s
Epoch: 12 cost time: 139.87289690971375
579
579
579
1109
Epoch: 12, Steps: 113 | Train Loss: 0.1151736 Vali Loss: 0.2900665 Test Loss: 0.2635418
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0007489971263499759
3714
3714
3714
	iters: 100, epoch: 13 | loss: 0.1081244
	speed: 2.5400s/iter; left time: 2044.6771s
Epoch: 13 cost time: 138.9847583770752
579
579
579
1109
Epoch: 13, Steps: 113 | Train Loss: 0.1144969 Vali Loss: 0.2935480 Test Loss: 0.2668000
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0006282917303829499
3714
3714
3714
	iters: 100, epoch: 14 | loss: 0.1471484
	speed: 2.5138s/iter; left time: 1739.5651s
Epoch: 14 cost time: 138.0245144367218
579
579
579
1109
Epoch: 14, Steps: 113 | Train Loss: 0.1144711 Vali Loss: 0.2919948 Test Loss: 0.2657424
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0004988436012991294
3714
3714
3714
	iters: 100, epoch: 15 | loss: 0.1171572
	speed: 2.4898s/iter; left time: 1441.5888s
Epoch: 15 cost time: 137.36314964294434
579
579
579
1109
Epoch: 15, Steps: 113 | Train Loss: 0.1132817 Vali Loss: 0.2946250 Test Loss: 0.2603443
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00036947441517242894
3714
3714
3714
	iters: 100, epoch: 16 | loss: 0.0978272
	speed: 2.7911s/iter; left time: 1300.6333s
Epoch: 16 cost time: 172.25538802146912
579
579
579
1109
Epoch: 16, Steps: 113 | Train Loss: 0.1126060 Vali Loss: 0.2907485 Test Loss: 0.2681251
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0002490004682446946
3714
3714
3714
	iters: 100, epoch: 17 | loss: 0.1310225
	speed: 3.6756s/iter; left time: 1297.5028s
Epoch: 17 cost time: 263.8270628452301
579
579
579
1109
Epoch: 17, Steps: 113 | Train Loss: 0.1121394 Vali Loss: 0.2975087 Test Loss: 0.2676173
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00014563186090644047
3714
3714
3714
	iters: 100, epoch: 18 | loss: 0.1183453
	speed: 5.0551s/iter; left time: 1213.2241s
Epoch: 18 cost time: 333.621365070343
579
579
579
1109
Epoch: 18, Steps: 113 | Train Loss: 0.1106425 Vali Loss: 0.3028261 Test Loss: 0.2679108
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.641299292306835e-05
3714
3714
3714
	iters: 100, epoch: 19 | loss: 0.1137035
	speed: 4.8507s/iter; left time: 616.0435s
Epoch: 19 cost time: 347.39329290390015
579
579
579
1109
Epoch: 19, Steps: 113 | Train Loss: 0.1111103 Vali Loss: 0.3002114 Test Loss: 0.2678975
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : finance_cipla_48_48_PathFormer_CIPLA_ftMS_sl48_pl48_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
 DATA LOADED FROM DATA_LOADER 
          date  Prev Close    Open  ...  Volume      Turnover    Close
0  03-01-2000     1349.40  1410.0  ...   21060  3.035500e+12  4372.05
1  04-01-2000     1457.35  1537.0  ...   30215  4.412700e+12  4395.75
2  05-01-2000     1465.25  1474.0  ...   33799  4.826870e+12  4305.15
3  06-01-2000     1435.05  1434.0  ...   33083  4.600360e+12  4067.55
4  07-01-2000     1355.85  1370.0  ...   66536  8.433350e+12  3742.65

[5 rows x 10 columns]
after adjusting cols 
      Prev Close     Open     High  ...    Volume      Turnover      Close
0        1349.40  1410.00  1457.35  ...     21060  3.035500e+12   4372.050
1        1457.35  1537.00  1537.00  ...     30215  4.412700e+12   4395.750
2        1465.25  1474.00  1474.00  ...     33799  4.826870e+12   4305.150
3        1435.05  1434.00  1435.00  ...     33083  4.600360e+12   4067.550
4        1355.85  1370.00  1389.90  ...     66536  8.433350e+12   3742.650
...          ...      ...      ...  ...       ...           ...        ...
5301      935.60   935.60   940.00  ...  10255697  9.374880e+14  33952.500
5302      905.40   913.00   919.50  ...   5669049  5.154420e+14  34215.000
5303      912.40   914.35   918.00  ...   7251009  6.597970e+14  34132.500
5304      910.20   911.95   917.40  ...   4953091  4.508420e+14  33993.750
5305      906.50   900.75   921.00  ...   6459737  5.887820e+14  34138.125

[5306 rows x 9 columns]
after scaling the data 
 [[ 2.47872585  2.65155099  2.71669996 ... -0.90224236 -0.81706707
  -0.79786747]
 [ 2.7993685   3.02807816  2.94856278 ... -0.89342097 -0.77846687
  -0.7928865 ]
 [ 2.82283377  2.84129696  2.76516846 ... -0.88996757 -0.7668585
  -0.81192768]
 ...
 [ 1.18070979  1.18205734  1.14664081 ...  6.06424465 17.59066244
   5.4568124 ]
 [ 1.17417515  1.17494187  1.1448942  ...  3.85006363 11.73406592
   5.42765166]
 [ 1.16318508  1.14173632  1.15537387 ...  5.30180689 15.60025206
   5.45799459]]
1109
test 1014
x data after scaling 
[[ 9.16535136e-02  1.00949991e-01  7.80050156e-02 ...  5.59964999e-02
   6.26802382e-01  2.50211982e+00]
 [ 6.06139989e-02 -1.05257704e-02  2.29867195e-02 ...  2.74955037e+00
   4.68126119e+00  2.43118829e+00]
 [ 3.38814025e-02  8.70155207e-02  1.27783474e-01 ...  2.91511786e+00
   5.34423616e+00  2.72594823e+00]
 ...
 [ 1.18070979e+00  1.18205734e+00  1.14664081e+00 ...  6.06424465e+00
   1.75906624e+01  5.45681240e+00]
 [ 1.17417515e+00  1.17494187e+00  1.14489420e+00 ...  3.85006363e+00
   1.17340659e+01  5.42765166e+00]
 [ 1.16318508e+00  1.14173632e+00  1.15537387e+00 ...  5.30180689e+00
   1.56002521e+01  5.45799459e+00]]
len of sample data 9

1109
(992, 48, 1)
(992, 48, 1)
1.3629322871565819
mse:190884.453125, mae:296.625, rse:0.12723387777805328 ,mape : 1.3629322871565819
Inference time:  112.07964587211609
